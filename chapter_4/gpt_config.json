{
    "GPT_CONFIG_124M": {
        "vocab_size": 50257,
        "context_length": 256,
        "emb_dim": 768,
        "n_heads": 12,
        "n_layers": 12, 
        "drop_rate_att": 0.2,
        "drop_rate_shortcut": 0.1,
        "drop_rate_emb": 0.15,
        "qkv_bias": false 
    }
}